Metadata-Version: 2.1
Name: buildflow
Version: 0.0.1
Summary: flow cli application
Author-email: Caleb Van Dyke <caleb@launchflow.com>, Josh Tanke <josh@launchflow.com>
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: apache-beam
Requires-Dist: duckdb (==0.6.0)
Requires-Dist: google-cloud-bigquery
Requires-Dist: google-cloud-bigquery-storage
Requires-Dist: google-cloud-pubsub
Requires-Dist: grpcio (==1.48.2)
Requires-Dist: opentelemetry-api
Requires-Dist: opentelemetry-sdk
Requires-Dist: opentelemetry-exporter-otlp
Requires-Dist: opentelemetry-exporter-jaeger
Requires-Dist: pandas
Requires-Dist: ray
Requires-Dist: redis
Provides-Extra: dev
Requires-Dist: pytest ; extra == 'dev'
Requires-Dist: flake8 ; extra == 'dev'
Requires-Dist: setuptools ; extra == 'dev'
Requires-Dist: wheel ; extra == 'dev'

# buildflow

![CI](https://github.com/launchflow/buildflow/actions/workflows/python_ci.yaml/badge.svg)

**buildflow** is a unified **batch** and **streaming** framework that turns
any python function into a scalable data pipeline.

Key Features:

- Fast - Scalable multiprocessing powered by [Ray](https://ray.io)
- Easy to learn- Get started with 2 lines of code
- Production Ready - Ready made IO connectors let users focus on processing
  data instead of reading and writing data

## Quick Start

Install the framework

```
pip install buildflow
```

Import the framework.

```python
import buildflow as flow
```

Add the `flow.processor` decorator to your function to attach IO.

```python
QUERY = 'SELECT * FROM `table`'
@flow.processor(input_ref=flow.BigQuery(query=QUERY))
def process(bigquery_row):
    ...
```

Use `flow.run()` to kick off your pipeline.

```python
flow.run()
```

## Examples

Streaming pipeline reading from Google Pub/Sub and writing to BigQuery.

```python
# Turn your function into a stream processor
@flow.processor(
   input_ref=flow.PubSub(subscription_id='my_subscription'),
   output_ref=flow.BigQuery(table_id='project.dataset.table'),
)
def stream_process(pubsub_message):
   ...

flow.run()

```

Batch pipeline reading and writing to BigQuery.

```python
import buildflow as flow

QUERY = 'SELECT * FROM `project.dataset.input_table`'
@flow.processor(
    input_ref=flow.BigQuery(query=QUERY),
    output_ref=flow.BigQuery(table_id='project.dataset.output_table'),
)
def process(bigquery_row):
    ...

flow.run()
```

Batch pipeline reading from BigQuery and returning output locally.

```python
import buildflow as flow

QUERY = 'SELECT * FROM `table`'
@flow.processor(input_ref=flow.BigQuery(query=QUERY))
def process(bigquery_row):
    ...

processed_rows = flow.run()
```
